# A feedfoward neural network with a hidden layer, back propogation, and online learning
FeedforwardNetwork :: struct {
  learning_rate: float;
  hidden: [2]HiddenNode;
  outputs: [1]OutputNode;
}

HiddenNode :: struct {
  output: float;
  bias: float;
  bias_delta: float;
  weights: [1]float;
  weight_deltas: [1]float;
}

OutputNode :: struct {
  bias: float;
  bias_delta: float;
  weights: [2]float;
  weight_deltas: [2]float;
}

train_feedforward :: (net: ^FeedforwardNetwork, inputs: [1]float, outputs: [1]float) {
  hidden_outputs : [2] float;
  for node, index in net.hidden:
    # 1. Compute net input and output for hidden nodes (tanh)
    hidden_outputs[index] = Math.tanh(net_input(inputs, node.weights));

  output_errors : [1] float;
  for node, index in net.outputs {
    # 2. Compute net input and ouput for output nodes (identity)
    output := net_input(net.inputs, node.weights);

    # 3. Compute error for outputs nodes
    error := outputs[index] - output;
    output_errors[index] = error;

    # 4. Update weights for output nodes (online learning)
    node.bias_delta = node.bias_delta + error;
    node.bias = node.bias + (node.bias_delta * net.learning_rate);
    for weight, index in node.weights {
      weight_delta := node.weight_deltas[index] + (error * hidden_outputs[index]);
      node.weights[index] = weight + (weight_delta * net.learning_rate);
      node.weight_deltas[index] = weight_delta;
    }
  }

  for ^node, index in net.hidden {
    # 5. Compute error for hidden nodes
    error := tanh_error(node.output, errors, node.weights);

    # 6. Update weights for hidden nodes (online learning)
    node.bias_delta = node.bias_delta + error;
    node.bias = node.bias + (node.bias_delta * net.learning_rate);
    for weight, index in node.weights {
      weight_delta := node.weight_deltas[index] + (error * inputs[index]);
      node.weights[index] = weight + (weight_delta * net.learning_rate);
      node.weight_deltas[index] = weight_delta;
    }
  }
}

net_input :: (inputs: []float, weights: []float) -> float {
  result := 0.0;
  for input, index in inputs:
    result += input * weights[index];
  return result;
}

tanh_error :: (output: float, errors: []float, weights: []float) -> float {
  result := 0.0;
  for error, index in errors:
    result += input * weights[index];
  return result * (1 - output * output);
}

ExamplePattern :: struct {
  inputs: []float;
  outputs: []float;
}

examples : []ExamplePattern;
#- not really sure what the best syntax for array and struct "literals" is
examples : []ExamplePattern = [
  {[ 0.4,  0.1,  0.2, 0.7, 0.3], [0.1, 0.2, 0.3, 0.4, 0.5]},
  {[0.25, 0.35, 0.45, 0.6, 1.1], [0.1, 0.2, 0.3, 0.4, 0.5]},
];
-#

main :: () {
  neural_net : FeedforwardNetwork;
  for example in examples:
    train_feedforward(^neural_net, example.inputs, example.outputs);
}
