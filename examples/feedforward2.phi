net_input :: fn(inputs: []Float, weights: []Float) -> Float:
  result := 0.0;
  for input, index in inputs:
    result += input * weights[index];
  return result;
}

tanh_error :: fn(output: Float, errors: []Float, weights: []Float) -> Float {
  result := 0.0;
  for error, index in errors:
    result += error * weights[index];
  return result * (1 - output * output);
}

feedforward :: module {
  HiddenNode :: struct {
    output: Float;
    bias: Float;
    bias_delta: Float;
    weights: [1]Float;
    weight_deltas: [1]Float;
  }

  OutputNode :: struct {
    bias: Float;
    bias_delta: Float;
    weights: [2]Float;
    weight_deltas: [2]Float;
  }

  # a feedfoward neural network with a hidden layer, back propogation, and online learning
  Network :: struct {
    learning_rate: Float;
    hidden: [2]HiddenNode;
    outputs: [1]OutputNode;
  }
}

train_feedforward :: fn(net: ^feedforward.Network, inputs: [1]Float, outputs: [1]Float) {
  hidden_outputs : [2] Float;
  for node, index in net.hidden
    # 1. Compute net input and output for hidden nodes (tanh)\
    hidden_outputs[index] = math_tanh(net_input(inputs, node.weights));

  outputErrors : [1] Float;
  for node, index in net.outputs {
    # 2. Compute net input and ouput for output nodes (identity)
    output := net_input(net.inputs, node.weights);

    # 3. Compute error for outputs nodes
    error := outputs[index] - output;
    outputErrors[index] = error;

    # 4. Update weights for output nodes (online learning)
    node.bias_delta = node.bias_delta + error;
    node.bias = node.bias + (node.bias_delta * net.learning_rate);
    for weight, index in node.weights {
      weight_delta := node.weight_deltas[index] + (error * hidden_outputs[index]);
      node.weights[index] = weight + (weight_delta * net.learning_rate);
      node.weight_deltas[index] = weight_delta;
    }
  }

  for ^node, index in net.hidden {
    # 5. Compute error for hidden nodes
    error := tanh_error(node.output, errors, node.weights);

    # 6. Update weights for hidden nodes (online learning)
    node.bias_delta = node.bias_delta + error;
    node.bias = node.bias + (node.bias_delta * net.learning_rate);
    for weight, index in node.weights {
      weight_delta := node.weight_deltas[index] + (error * inputs[index]);
      node.weights[index] = weight + (weight_delta * net.learning_rate);
      node.weight_deltas[index] = weight_delta;
    }
  }
}

ExamplePattern :: struct {
  inputs: []Float;
  outputs: []Float;
}

#- not Floatly sure what the best syntax for array and struct "literals" is
examples : []ExamplePattern = [
  {[ 0.4,  0.1,  0.2, 0.7, 0.3], [0.1, 0.2, 0.3, 0.4, 0.5]},
  {[0.25, 0.35, 0.45, 0.6, 1.1], [0.1, 0.2, 0.3, 0.4, 0.5]},
];
-#
examples : []ExamplePattern;

main :: () {
  neural_net : feedforward.Network;
  for example in examples:
    train_feedforward(^neural_net, example.inputs, example.outputs);
}
